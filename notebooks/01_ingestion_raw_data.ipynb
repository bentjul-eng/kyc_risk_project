{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716be63e-305e-44aa-9f8e-385631f7776b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# 01_ingestion.py\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Parâmetro para input/output path\n",
    "dbutils.widgets.text(\"input_path\", \"/mnt/kycproject/raw_data\")\n",
    "input_path = dbutils.widgets.get(\"input_path\")\n",
    "\n",
    "print(f\"Processando dados em: {input_path}\")\n",
    "\n",
    "# Criar diretório se não existir\n",
    "dbutils.fs.mkdirs(input_path)\n",
    "\n",
    "# --- Database ---\n",
    "names = [\n",
    "    'Carlos L.', 'Sofia R.', 'Lucas M.', 'Isabela F.', 'Gabriel A.',\n",
    "    'Laura C.', 'Mateus G.', 'Júlia S.', 'Pedro R.', 'Beatriz S.',\n",
    "    'John S.', 'Maria G.', 'James J.', 'Patricia B.', 'Robert J.',\n",
    "    'Wei C.', 'Li N.', 'Jing W.', 'Yang L.', 'Wei Z.',\n",
    "    'Fatima A.', 'Mohammed A.', 'Ahmed A.', 'Aisha A.'\n",
    "]\n",
    "\n",
    "countries = [\n",
    "    'Brazil', 'Portugal', 'Spain', 'China', 'Oman',\n",
    "    'Germany', 'India', 'Russia', 'Argentina', 'Mexico',\n",
    "    'Iran', 'Venezuela', 'Myanmar', 'Lebanon', 'North Korea'\n",
    "]\n",
    "\n",
    "age_range = (15, 75)\n",
    "\n",
    "# --- Generate clients ---\n",
    "n_clients = 150\n",
    "clients = []\n",
    "for i in range(1, n_clients + 1):\n",
    "    clients.append((\n",
    "        i,\n",
    "        random.choice(names),\n",
    "        random.randint(*age_range),\n",
    "        random.choice(countries)\n",
    "    ))\n",
    "\n",
    "clients_schema = StructType([\n",
    "    StructField(\"client_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"country\", StringType(), False)\n",
    "])\n",
    "\n",
    "clients_df = spark.createDataFrame(clients, schema=clients_schema)\n",
    "\n",
    "clients_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(f\"{input_path}/clients.csv\")\n",
    "\n",
    "# --- Generate transactions ---\n",
    "n_transactions = 500\n",
    "transactions = []\n",
    "for i in range(1, n_transactions + 1):\n",
    "    client = random.choice(clients)\n",
    "    amount = round(random.uniform(10.0, 5000.0), 2)\n",
    "    days_ago = random.randint(0, 365)\n",
    "    date = (datetime.now() - timedelta(days=days_ago)).date()\n",
    "\n",
    "    transactions.append((\n",
    "        i,\n",
    "        client[0],  # client_id\n",
    "        amount,\n",
    "        date.isoformat()\n",
    "    ))\n",
    "\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), False),\n",
    "    StructField(\"client_id\", IntegerType(), False),\n",
    "    StructField(\"transaction_amount\", FloatType(), False),\n",
    "    StructField(\"transaction_date\", StringType(), False)\n",
    "])\n",
    "\n",
    "transactions_df = spark.createDataFrame(transactions, schema=transactions_schema)\n",
    "transactions_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(f\"{input_path}/transactions.csv\")\n",
    "\n",
    "# --- Generate high_risk_countries ---\n",
    "n_high_risk_countries = 10\n",
    "high_risk_sample = random.sample(countries, n_high_risk_countries)\n",
    "high_risk = [(c,) for c in high_risk_sample]\n",
    "\n",
    "high_risk_schema = StructType([\n",
    "    StructField(\"high_risk_country\", StringType(), False)\n",
    "])\n",
    "\n",
    "high_risk_df = spark.createDataFrame(high_risk, schema=high_risk_schema)\n",
    "high_risk_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(f\"{input_path}/high_risk_countries.csv\")\n",
    "\n",
    "print(\"Arquivos CSV gerados com sucesso.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingestion_raw_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
