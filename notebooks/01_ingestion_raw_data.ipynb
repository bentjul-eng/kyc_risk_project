{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "716be63e-305e-44aa-9f8e-385631f7776b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# --- Parameters ---\n",
    "n_clients = 150\n",
    "n_transactions = 500\n",
    "n_high_risk_countries = 10\n",
    "\n",
    "# --- Paths ---\n",
    "output_path = \"dbfs:/Workspace/Users/jusoares_flor@hotmail.com/kyc_risk_project/data/csv_sources\"\n",
    "dbutils.fs.mkdirs(output_path)\n",
    "\n",
    "# --- Database ---\n",
    "names = [\n",
    "    'Carlos L.', 'Sofia R.', 'Lucas M.', 'Isabela F.', 'Gabriel A.',\n",
    "    'Laura C.', 'Mateus G.', 'Júlia S.', 'Pedro R.', 'Beatriz S.',\n",
    "    'John S.', 'Maria G.', 'James J.', 'Patricia B.', 'Robert J.',\n",
    "    'Wei C.', 'Li N.', 'Jing W.', 'Yang L.', 'Wei Z.',\n",
    "    'Fatima A.', 'Mohammed A.', 'Ahmed A.', 'Aisha A.'\n",
    "]\n",
    "\n",
    "countries = [\n",
    "    'Brazil', 'Portugal', 'Spain', 'China', 'Oman',\n",
    "    'Germany', 'India', 'Russia', 'Argentina', 'Mexico',\n",
    "    'Iran', 'Venezuela', 'Myanmar', 'Lebanon', 'North Korea'\n",
    "]\n",
    "\n",
    "age_range = (15, 75)\n",
    "\n",
    "# --- Generate clients ---\n",
    "clients = []\n",
    "for i in range(1, n_clients + 1):\n",
    "    clients.append((\n",
    "        i,\n",
    "        random.choice(names),\n",
    "        random.randint(*age_range),\n",
    "        random.choice(countries)\n",
    "    ))\n",
    "\n",
    "# Define esquema para o dataframe de clientes\n",
    "clients_schema = StructType([\n",
    "    StructField(\"client_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"country\", StringType(), False)\n",
    "])\n",
    "\n",
    "clients_df = spark.createDataFrame(clients, schema=clients_schema)\n",
    "\n",
    "# Salvar CSV - para salvar exatamente como CSV simples, desabilitar cabeçalho na pasta e salvar particionado, usar modo overwrite\n",
    "clients_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(f\"{output_path}/clients.csv\")\n",
    "\n",
    "# --- Generate transactions ---\n",
    "transactions = []\n",
    "for i in range(1, n_transactions + 1):\n",
    "    client = random.choice(clients)\n",
    "    amount = round(random.uniform(10.0, 50000.0), 2)\n",
    "    days_ago = random.randint(0, 365)\n",
    "    date = (datetime.now() - timedelta(days=days_ago)).date()\n",
    "\n",
    "    transactions.append((\n",
    "        i,\n",
    "        client[0],  # client_id\n",
    "        amount,\n",
    "        date.isoformat()\n",
    "    ))\n",
    "\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), False),\n",
    "    StructField(\"client_id\", IntegerType(), False),\n",
    "    StructField(\"transaction_amount\", FloatType(), False),\n",
    "    StructField(\"transaction_date\", StringType(), False)\n",
    "])\n",
    "\n",
    "transactions_df = spark.createDataFrame(transactions, schema=transactions_schema)\n",
    "transactions_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(f\"{output_path}/transactions.csv\")\n",
    "\n",
    "# --- Generate high_risk_countries ---\n",
    "high_risk_sample = random.sample(countries, n_high_risk_countries)\n",
    "high_risk = [(c,) for c in high_risk_sample]\n",
    "\n",
    "high_risk_schema = StructType([\n",
    "    StructField(\"high_risk_country\", StringType(), False)\n",
    "])\n",
    "\n",
    "high_risk_df = spark.createDataFrame(high_risk, schema=high_risk_schema)\n",
    "high_risk_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(f\"{output_path}/high_risk_countries.csv\")\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingestion_raw_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
